{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard","accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["Initialize repository, copy weights from Google drive."],"metadata":{"id":"uh_RON9_FqBV"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"aChaziGm-OrN","colab":{"base_uri":"https://localhost:8080/"},"outputId":"bfd54106-5fc5-4c2f-a5f2-948fee353767","executionInfo":{"status":"ok","timestamp":1713436999324,"user_tz":-60,"elapsed":116005,"user":{"displayName":"Beatriz Salvador","userId":"06705657987229056123"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /drive\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.3/266.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for fairscale (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Cloning into 'llama'...\n","remote: Enumerating objects: 460, done.\u001b[K\n","remote: Counting objects: 100% (43/43), done.\u001b[K\n","remote: Compressing objects: 100% (30/30), done.\u001b[K\n","remote: Total 460 (delta 15), reused 31 (delta 12), pack-reused 417\u001b[K\n","Receiving objects: 100% (460/460), 1.11 MiB | 29.26 MiB/s, done.\n","Resolving deltas: 100% (233/233), done.\n","Thu Apr 18 10:43:17 2024       \n","+---------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n","|-----------------------------------------+----------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                      |               MIG M. |\n","|=========================================+======================+======================|\n","|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n","| N/A   60C    P8              10W /  70W |      0MiB / 15360MiB |      0%      Default |\n","|                                         |                      |                  N/A |\n","+-----------------------------------------+----------------------+----------------------+\n","                                                                                         \n","+---------------------------------------------------------------------------------------+\n","| Processes:                                                                            |\n","|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n","|        ID   ID                                                             Usage      |\n","|=======================================================================================|\n","|  No running processes found                                                           |\n","+---------------------------------------------------------------------------------------+\n"]}],"source":["import os\n","import sys\n","from google.colab import drive\n","\n","# Mount google drive.\n","drive.mount('/drive')\n","\n","#@markdown Location of tokenizer.\n","tokenizer_loc = '/drive/MyDrive/Colab Notebooks/ISO/llama/tokenizer/tokenizer.model' #@param {type:\"string\"}\n","\n","# @markdown Location of directory containing model weights / parameters.\n","weight_loc = '/drive/MyDrive/Colab Notebooks/ISO/llama/7B-chat/' #@param {type:\"string\"}\n","\n","!pip install -q fairscale sentencepiece\n","!git clone https://github.com/facebookresearch/llama.git\n","\n","sys.path.insert(0, '/content/llama/')\n","\n","!nvidia-smi"]},{"cell_type":"code","source":["!pip install -q -r '/content/llama/requirements.txt'"],"metadata":{"id":"z4-0n2_exf-n","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1713437007263,"user_tz":-60,"elapsed":7948,"user":{"displayName":"Beatriz Salvador","userId":"06705657987229056123"}},"outputId":"4b77998d-a2a7-4bf8-a070-c6b58fbb37c1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/88.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.4/88.4 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}]},{"cell_type":"markdown","source":["The 7B checkpoint is too large to fit into RAM. Run this cell if you need to split the 7B checkpoint. Will save the results to your 7B directory so you should only ever need to run this cell once. You may need to restart the runtime afterward."],"metadata":{"id":"YUViS0koD_aj"}},{"cell_type":"code","source":["import torch\n","\n","# @markdown Choose if to split the model or if the split checkpoints are already created.\n","SPLIT = False #@param\n","\n","if SPLIT:\n","    checkpoint = torch.load(os.path.join(weight_loc, 'consolidated.00.pth'),\n","                            map_location=\"cuda\")\n","\n","    d1 = dict(list(checkpoint.items())[:len(checkpoint)//2])\n","    torch.save(d1, os.path.join(weight_loc, 'consolidated.00.00.pth'))\n","    del(d1)\n","\n","    d2 = dict(list(checkpoint.items())[len(checkpoint)//2:])\n","    torch.save(d2, os.path.join(weight_loc, 'consolidated.00.01.pth'))\n","    del(d2)\n","\n","    del(checkpoint)"],"metadata":{"id":"MxwFpC1fCAdz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Prepare loading"],"metadata":{"id":"k0I2vkJ-cOr5"}},{"cell_type":"code","source":["from typing import List, Literal, Optional, Tuple, TypedDict\n","import os\n","import sys\n","import torch\n","import time\n","import json\n","from tqdm import tqdm\n","import pandas as pd\n","\n","from pathlib import Path\n","\n","import torch.nn.functional as F\n","from fairscale.nn.model_parallel.initialize import (\n","    get_model_parallel_rank,\n","    initialize_model_parallel,\n","    model_parallel_is_initialized,\n",")\n","\n","from llama.model import ModelArgs, Transformer\n","from llama.tokenizer import Tokenizer\n","from llama.generation import Llama"],"metadata":{"id":"82rq7aa4b-dU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["os.environ['RANK'] = '0'\n","os.environ['WORLD_SIZE'] = '1'\n","os.environ['MP'] = '1'\n","os.environ['MASTER_ADDR'] = '127.0.0.1'\n","os.environ['MASTER_PORT'] = '2223'\n","\n","local_rank = int(os.environ.get(\"LOCAL_RANK\", -1))\n","world_size = int(os.environ.get(\"WORLD_SIZE\", -1))\n","\n","torch.distributed.init_process_group(\"gloo\")\n","initialize_model_parallel(world_size)\n","torch.cuda.set_device(local_rank)\n","\n","# seed must be the same in all processes\n","torch.manual_seed(42)\n","\n","if local_rank > 0:\n","    sys.stdout = open(os.devnull, 'w')\n","\n","# @markdown Context size. Can be up to 2048, but Colab GPU doesn't always play well with high values.\n","max_seq_len = 256 # @param {type:\"number\"}\n","# @markdown Maximum batch size. Recommended to keep it low.\n","batch_size = 4 # @param {type:\"number\"}"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m_OFJ9m0cGEG","executionInfo":{"status":"ok","timestamp":1713355542467,"user_tz":-60,"elapsed":16,"user":{"displayName":"Beatriz Salvador","userId":"06705657987229056123"}},"outputId":"0b3220d2-a13b-421e-e501-43b48f5d45a7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["> initializing model parallel with size 1\n","> initializing ddp with size 1\n","> initializing pipeline with size 1\n"]}]},{"cell_type":"markdown","source":["Load model."],"metadata":{"id":"47yXVODMO6l0"}},{"cell_type":"code","source":["def build_llama(\n","        ckpt_dir: str,\n","        tokenizer_path: str,\n","        max_seq_len: int,\n","        max_batch_size: int,\n","        model_parallel_size: Optional[int] = None,\n","        seed: int = 1,\n","    ) -> \"Llama\":\n","        \"\"\"\n","        Build a Llama instance by initializing and loading a pre-trained model.\n","\n","        Args:\n","            ckpt_dir (str): Path to the directory containing checkpoint files.\n","            tokenizer_path (str): Path to the tokenizer file.\n","            max_seq_len (int): Maximum sequence length for input text.\n","            max_batch_size (int): Maximum batch size for inference.\n","            model_parallel_size (Optional[int], optional): Number of model parallel processes.\n","                If not provided, it's determined from the environment. Defaults to None.\n","\n","        Returns:\n","            Llama: An instance of the Llama class with the loaded model and tokenizer.\n","\n","        Raises:\n","            AssertionError: If there are no checkpoint files in the specified directory,\n","                or if the model parallel size does not match the number of checkpoint files.\n","\n","        Note:\n","            This method initializes the distributed process group, sets the device to CUDA,\n","            and loads the pre-trained model and tokenizer.\n","\n","        \"\"\"\n","        start_time = time.time()\n","\n","        print(\"Loading\")\n","        with open(Path(ckpt_dir) / \"params.json\", \"r\") as f:\n","            params = json.loads(f.read())\n","\n","        model_args: ModelArgs = ModelArgs(max_seq_len=max_seq_len,\n","                                            max_batch_size=max_batch_size,\n","                                            **params)\n","        tokenizer = Tokenizer(model_path=tokenizer_path)\n","        model_args.vocab_size = tokenizer.n_words\n","        torch.set_default_tensor_type(torch.cuda.HalfTensor)\n","        model = Transformer(model_args).cuda().half()\n","        torch.set_default_tensor_type(torch.FloatTensor)\n","\n","        checkpoint_paths = [os.path.join(weight_loc, 'consolidated.00.00.pth'),\n","                            os.path.join(weight_loc, 'consolidated.00.01.pth')]\n","\n","        for checkpoint_path in checkpoint_paths:\n","            checkpoint = torch.load(checkpoint_path, map_location='cpu')\n","            model.load_state_dict(checkpoint, strict=False)\n","            del checkpoint\n","\n","        generator = Llama(model=model, tokenizer=tokenizer)\n","\n","        print(f\"Loaded in {time.time() - start_time:.2f} seconds\")\n","        return generator"],"metadata":{"id":"V5RpCXv3w9sc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["generator = build_llama(\n","        ckpt_dir=weight_loc,\n","        tokenizer_path=tokenizer_loc,\n","        max_seq_len=max_seq_len,\n","        max_batch_size=batch_size,\n","        )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uKLNf3GvyCbg","executionInfo":{"status":"ok","timestamp":1713355841578,"user_tz":-60,"elapsed":231944,"user":{"displayName":"Beatriz Salvador","userId":"06705657987229056123"}},"outputId":"8f5db5c4-117b-4336-993c-adbb3787fd93"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/__init__.py:696: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:451.)\n","  _C._set_default_tensor_type(t)\n"]},{"output_type":"stream","name":"stdout","text":["Loaded in 231.76 seconds\n"]}]},{"cell_type":"code","source":["prompts: List[str] = [\n","        # For these prompts, the expected answer is the natural continuation of the prompt\n","        \"I believe the meaning of life is\",\n","        \"Simply put, the theory of relativity states that \",\n","        \"\"\"A brief message congratulating the team on the launch:\n","\n","        Hi everyone,\n","\n","        I just \"\"\",\n","        # Few shot prompt (providing a few examples before asking model to complete more);\n","        \"\"\"Translate English to French:\n","\n","        sea otter => loutre de mer\n","        peppermint => menthe poivrée\n","        plush girafe => girafe peluche\n","        cheese =>\"\"\",\n","    ]\n","\n","results = generator.text_completion(\n","        prompts[:batch_size],\n","        max_gen_len=220,\n","        temperature=1,\n","        top_p=0.9,\n","    )\n","\n","results"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jl_03rvKz-rV","executionInfo":{"status":"ok","timestamp":1713355860781,"user_tz":-60,"elapsed":19218,"user":{"displayName":"Beatriz Salvador","userId":"06705657987229056123"}},"outputId":"71951708-4985-42a7-bb92-7328d9e8e50b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'generation': 'to be kind to one another and try to alleviate the suffering of our fellow creatures wherever and whenever we can.\\nThe next biggest asset after intelligence is mental toughness.\\nMy greatest wish is that I may become so absorbed in the work of the Lord that I may never know anything of pain or sorrow.\\nChrist is the supreme artist because He created a sense of love that helps us see beyond the surface and into the core of another person’s soul.\\nDon’t be content with simplicity, but seek inspiration in the wonders of nature.\\nSometimes you have to put yourself in your worst position in order to see that your lowest point is not so low after all.\\nOur greatest glory is not in never failing, but in rising up every time we fall.\\nEven the richest among us are no more than stewards of the wealth God has given us.\\nIs not the beautiful and precious book of Nature the grandest epic?\\nIt’s a fact that most men die before they are fully born'},\n"," {'generation': \"1) the speed of light is constant, and 2) nothing can travel faster than the speed of light.\\nHowever, as far as I know, there is no possible way to directly observe the speed of light and compare it to your own speed in a meaningful way. In other words, nothing traveling in the universe (in our part of the universe, anyway) is likely to be traveling at the speed of light.\\nThis theory has to be taken on faith.\\nNot the answer you're looking for? Browse other questions tagged speed-of-light relativity or ask your own question.\\nSpeed of light constant?\\nWhat is the probability of light passing through an elongated hole?\\nWhat is the maximum speed attainable using relativistic rockets?\\nCalculating local speed of light using spacecraft speed measurements and relativity\\nHow would one traveling at high speed to the edges of the universe have any effect on my universe?\\nHow does the Special Relativity Theory view a human being?\"},\n"," {'generation': 'launched it [one-pager](https://dev.to/Burudkin/backbone-todos-v3-320l)\\n        and found several bugs, I hope everyone can test it. If there is any bugs, please send an email to 224243405@qq.com \\n        \\n        Good luck\\n        \\n        [Dev.to/Burudkin](https://dev.to/Burudkin)\\n\\nThis message will not be visible to other readers.\\n\\nFor further information and the source code please visit the [repository](https://github.com/mejustgotdrafted/backbone-todos)\\n\\n### Pre-launch and post-launch email\\n\\nI want to introduce our new app(ToDo) and welcome the early beta testers. \\nThe purpose of this pre-launch email is to gain early adopters and a small audience. I want to spread the word as much as possible so that I will have'},\n"," {'generation': 'fromage\\n        potatoes => pomme de terre\\n        ...\\n        \\n        Then:\\n        girafe.nom > \"plush girafe\"\\n        ou le résultat est une \"aversion\" pour la traduction'}]"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["cd '/drive/MyDrive/Colab Notebooks/ISO/instances'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ggZO-UI740ht","executionInfo":{"status":"ok","timestamp":1713355860782,"user_tz":-60,"elapsed":23,"user":{"displayName":"Beatriz Salvador","userId":"06705657987229056123"}},"outputId":"32479b2f-3b8e-43f8-9aef-df40004edba4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/drive/.shortcut-targets-by-id/1k2JFo63MNzskIgAHpOe33d-BrKTiK7mE/ISO/instances\n"]}]},{"cell_type":"code","source":["import string\n","p = set(string.printable)"],"metadata":{"id":"yvqg2uCX7Iq_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["filename = 'llama_no_watermark.json'\n","\n","df = pd.read_json('c4_selection.json', lines=True)\n","\n","with open(filename, \"r+\" if os.path.exists(filename) else \"w+\") as f:\n","    with torch.no_grad():\n","        for i in tqdm(range(len(f.readlines()), len(df[\"input\"]), batch_size)):\n","            line = df[\"input\"][i:i+batch_size].values.tolist()\n","\n","            out_text = generator.text_completion(\n","                                                        line,\n","                                                        max_gen_len=220,\n","                                                        temperature=1,\n","                                                        top_p=0.9,\n","                                                        logprobs=False\n","                                                    )\n","\n","            for j in range(len(out_text)):\n","                output_text = out_text[j][\"generation\"]\n","                _line = ''.join(filter(lambda x: x in p, line[j])).replace('\\n', '\\\\n').replace('\"', '\\\\\"')\n","                _output_text = ''.join(filter(lambda x: x in p, output_text)).replace('\\n', '\\\\n').replace('\"', '\\\\\"')\n","\n","                f.write(f'{{\"input\": \"{_line}\", \"continuation\": \"{_output_text}\"}}\\n')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bqkiwAH75CAZ","executionInfo":{"status":"ok","timestamp":1713357761306,"user_tz":-60,"elapsed":1821261,"user":{"displayName":"Beatriz Salvador","userId":"06705657987229056123"}},"outputId":"eb2fdb91-2606-4780-ffb8-faf6606d4c29"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 128/128 [30:20<00:00, 14.23s/it]\n"]}]}]}