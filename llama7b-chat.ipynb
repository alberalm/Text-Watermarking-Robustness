{"cells":[{"cell_type":"markdown","metadata":{"id":"uh_RON9_FqBV"},"source":["Initialize repository, copy weights from Google drive."]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":100495,"status":"ok","timestamp":1713986770048,"user":{"displayName":"ALBERTO ALMAGRO SÁNCHEZ","userId":"05614599307350314709"},"user_tz":-60},"id":"aChaziGm-OrN","outputId":"fbbfbfd5-fde0-4c24-b810-71f5dc4f30d6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /drive\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.3/266.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for fairscale (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Cloning into 'llama'...\n","remote: Enumerating objects: 460, done.\u001b[K\n","remote: Counting objects: 100% (43/43), done.\u001b[K\n","remote: Compressing objects: 100% (30/30), done.\u001b[K\n","remote: Total 460 (delta 15), reused 31 (delta 12), pack-reused 417\u001b[K\n","Receiving objects: 100% (460/460), 1.11 MiB | 10.87 MiB/s, done.\n","Resolving deltas: 100% (233/233), done.\n","Wed Apr 24 19:26:08 2024       \n","+---------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n","|-----------------------------------------+----------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                      |               MIG M. |\n","|=========================================+======================+======================|\n","|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n","| N/A   55C    P8              10W /  70W |      0MiB / 15360MiB |      0%      Default |\n","|                                         |                      |                  N/A |\n","+-----------------------------------------+----------------------+----------------------+\n","                                                                                         \n","+---------------------------------------------------------------------------------------+\n","| Processes:                                                                            |\n","|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n","|        ID   ID                                                             Usage      |\n","|=======================================================================================|\n","|  No running processes found                                                           |\n","+---------------------------------------------------------------------------------------+\n"]}],"source":["import os\n","import sys\n","from google.colab import drive\n","\n","# Mount google drive.\n","drive.mount('/drive')\n","\n","#@markdown Location of tokenizer.\n","tokenizer_loc = '/drive/MyDrive/Colab Notebooks/ISO/llama/tokenizer/tokenizer.model' #@param {type:\"string\"}\n","\n","# @markdown Location of directory containing model weights / parameters.\n","weight_loc = '/drive/MyDrive/Colab Notebooks/ISO/llama/7B-chat/' #@param {type:\"string\"}\n","\n","!pip install -q fairscale sentencepiece\n","!git clone https://github.com/facebookresearch/llama.git\n","\n","sys.path.insert(0, '/content/llama/')\n","\n","!nvidia-smi"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8397,"status":"ok","timestamp":1713986778438,"user":{"displayName":"ALBERTO ALMAGRO SÁNCHEZ","userId":"05614599307350314709"},"user_tz":-60},"id":"z4-0n2_exf-n","outputId":"c32ca660-e6ae-4b7e-f8e0-d0882d729ae7"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/88.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m81.9/88.4 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.4/88.4 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["!pip install -q -r '/content/llama/requirements.txt'"]},{"cell_type":"markdown","metadata":{"id":"YUViS0koD_aj"},"source":["The 7B checkpoint is too large to fit into RAM. Run this cell if you need to split the 7B checkpoint. Will save the results to your 7B directory so you should only ever need to run this cell once. You may need to restart the runtime afterward."]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":3814,"status":"ok","timestamp":1713986782247,"user":{"displayName":"ALBERTO ALMAGRO SÁNCHEZ","userId":"05614599307350314709"},"user_tz":-60},"id":"MxwFpC1fCAdz"},"outputs":[],"source":["import torch\n","\n","# @markdown Choose if to split the model or if the split checkpoints are already created.\n","SPLIT = False #@param\n","\n","if SPLIT:\n","    checkpoint = torch.load(os.path.join(weight_loc, 'consolidated.00.pth'),\n","                            map_location=\"cuda\")\n","\n","    d1 = dict(list(checkpoint.items())[:len(checkpoint)//2])\n","    torch.save(d1, os.path.join(weight_loc, 'consolidated.00.00.pth'))\n","    del(d1)\n","\n","    d2 = dict(list(checkpoint.items())[len(checkpoint)//2:])\n","    torch.save(d2, os.path.join(weight_loc, 'consolidated.00.01.pth'))\n","    del(d2)\n","\n","    del(checkpoint)"]},{"cell_type":"markdown","metadata":{"id":"k0I2vkJ-cOr5"},"source":["Prepare loading"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1713986782247,"user":{"displayName":"ALBERTO ALMAGRO SÁNCHEZ","userId":"05614599307350314709"},"user_tz":-60},"id":"82rq7aa4b-dU"},"outputs":[],"source":["from typing import List, Literal, Optional, Tuple, TypedDict\n","import os\n","import sys\n","import torch\n","import time\n","import json\n","from tqdm import tqdm\n","import pandas as pd\n","\n","from pathlib import Path\n","\n","import torch.nn.functional as F\n","from fairscale.nn.model_parallel.initialize import (\n","    get_model_parallel_rank,\n","    initialize_model_parallel,\n","    model_parallel_is_initialized,\n",")\n","\n","from llama.model import ModelArgs, Transformer\n","from llama.tokenizer import Tokenizer\n","from llama.generation import Llama, Dialog"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":434,"status":"ok","timestamp":1713986782675,"user":{"displayName":"ALBERTO ALMAGRO SÁNCHEZ","userId":"05614599307350314709"},"user_tz":-60},"id":"m_OFJ9m0cGEG","outputId":"fc24796a-e5f2-44b4-8d28-fc51614960a7"},"outputs":[{"output_type":"stream","name":"stdout","text":["> initializing model parallel with size 1\n","> initializing ddp with size 1\n","> initializing pipeline with size 1\n"]}],"source":["os.environ['RANK'] = '0'\n","os.environ['WORLD_SIZE'] = '1'\n","os.environ['MP'] = '1'\n","os.environ['MASTER_ADDR'] = '127.0.0.1'\n","os.environ['MASTER_PORT'] = '2223'\n","\n","local_rank = int(os.environ.get(\"LOCAL_RANK\", -1))\n","world_size = int(os.environ.get(\"WORLD_SIZE\", -1))\n","\n","torch.distributed.init_process_group(\"gloo\")\n","initialize_model_parallel(world_size)\n","torch.cuda.set_device(local_rank)\n","\n","# seed must be the same in all processes\n","torch.manual_seed(42)\n","\n","if local_rank > 0:\n","    sys.stdout = open(os.devnull, 'w')\n","\n","# @markdown Context size. Can be up to 2048, but Colab GPU doesn't always play well with high values.\n","max_seq_len = 1024 # @param {type:\"number\"}\n","# @markdown Maximum batch size. Recommended to keep it low.\n","batch_size = 2 # @param {type:\"number\"}"]},{"cell_type":"markdown","metadata":{"id":"47yXVODMO6l0"},"source":["Load model."]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1713986782675,"user":{"displayName":"ALBERTO ALMAGRO SÁNCHEZ","userId":"05614599307350314709"},"user_tz":-60},"id":"V5RpCXv3w9sc"},"outputs":[],"source":["def build_llama(\n","        ckpt_dir: str,\n","        tokenizer_path: str,\n","        max_seq_len: int,\n","        max_batch_size: int,\n","        model_parallel_size: Optional[int] = None,\n","        seed: int = 1,\n","    ) -> \"Llama\":\n","        \"\"\"\n","        Build a Llama instance by initializing and loading a pre-trained model.\n","\n","        Args:\n","            ckpt_dir (str): Path to the directory containing checkpoint files.\n","            tokenizer_path (str): Path to the tokenizer file.\n","            max_seq_len (int): Maximum sequence length for input text.\n","            max_batch_size (int): Maximum batch size for inference.\n","            model_parallel_size (Optional[int], optional): Number of model parallel processes.\n","                If not provided, it's determined from the environment. Defaults to None.\n","\n","        Returns:\n","            Llama: An instance of the Llama class with the loaded model and tokenizer.\n","\n","        Raises:\n","            AssertionError: If there are no checkpoint files in the specified directory,\n","                or if the model parallel size does not match the number of checkpoint files.\n","\n","        Note:\n","            This method initializes the distributed process group, sets the device to CUDA,\n","            and loads the pre-trained model and tokenizer.\n","\n","        \"\"\"\n","        start_time = time.time()\n","\n","        print(\"Loading\")\n","        with open(Path(ckpt_dir) / \"params.json\", \"r\") as f:\n","            params = json.loads(f.read())\n","\n","        model_args: ModelArgs = ModelArgs(max_seq_len=max_seq_len,\n","                                            max_batch_size=max_batch_size,\n","                                            **params)\n","        tokenizer = Tokenizer(model_path=tokenizer_path)\n","        model_args.vocab_size = tokenizer.n_words\n","        torch.set_default_tensor_type(torch.cuda.HalfTensor)\n","        model = Transformer(model_args).cuda().half()\n","        torch.set_default_tensor_type(torch.FloatTensor)\n","\n","        checkpoint_paths = [os.path.join(weight_loc, 'consolidated.00.00.pth'),\n","                            os.path.join(weight_loc, 'consolidated.00.01.pth')]\n","\n","        for checkpoint_path in checkpoint_paths:\n","            checkpoint = torch.load(checkpoint_path, map_location='cpu')\n","            model.load_state_dict(checkpoint, strict=False)\n","            del checkpoint\n","\n","        generator = Llama(model=model, tokenizer=tokenizer)\n","\n","        print(f\"Loaded in {time.time() - start_time:.2f} seconds\")\n","        return generator"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":253620,"status":"ok","timestamp":1713987036290,"user":{"displayName":"ALBERTO ALMAGRO SÁNCHEZ","userId":"05614599307350314709"},"user_tz":-60},"id":"uKLNf3GvyCbg","outputId":"a5596ded-ae32-4317-e45c-a576208629b2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loading\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/__init__.py:696: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:451.)\n","  _C._set_default_tensor_type(t)\n"]},{"output_type":"stream","name":"stdout","text":["Loaded in 253.66 seconds\n"]}],"source":["generator = build_llama(\n","        ckpt_dir=weight_loc,\n","        tokenizer_path=tokenizer_loc,\n","        max_seq_len=max_seq_len,\n","        max_batch_size=batch_size,\n","        )"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20,"status":"ok","timestamp":1713987036291,"user":{"displayName":"ALBERTO ALMAGRO SÁNCHEZ","userId":"05614599307350314709"},"user_tz":-60},"id":"ggZO-UI740ht","outputId":"ec5a6d4c-6c37-401f-c849-7950cb0b661e"},"outputs":[{"output_type":"stream","name":"stdout","text":["/drive/.shortcut-targets-by-id/1k2JFo63MNzskIgAHpOe33d-BrKTiK7mE/ISO/instances\n"]}],"source":["cd '/drive/MyDrive/Colab Notebooks/ISO/instances'"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":17,"status":"ok","timestamp":1713987036291,"user":{"displayName":"ALBERTO ALMAGRO SÁNCHEZ","userId":"05614599307350314709"},"user_tz":-60},"id":"bqkiwAH75CAZ"},"outputs":[],"source":["import string\n","p = set(string.printable)\n","\n","system_prompt = \"You are a paraphraser. You are given an input passage 'INPUT'. You should paraphrase 'INPUT' to print 'OUTPUT'. 'OUTPUT' shoud be diverse and different as much as possible from 'INPUT' and should not copy any part verbatim from 'INPUT'. 'OUTPUT' should preserve the meaning and content of 'INPUT' while maintaining text quality and grammar. 'OUTPUT' should not be much longer than 'INPUT'. You should print 'OUTPUT' and nothing else so that its easy for me to parse.\"\n","\n","def build_dialog(text: str) -> Dialog:\n","    return [{\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": \"INPUT: \" + text}]\n","\n","def build_paraphrase(original_filename, paraphrase_filename):\n","    df = pd.read_json(original_filename, lines=True)\n","\n","    with open(paraphrase_filename, \"r+\" if os.path.exists(paraphrase_filename) else \"w+\") as f:\n","        with torch.no_grad():\n","            for i in tqdm(range(len(f.readlines()), len(df[\"input\"]), batch_size)):\n","                input = df[\"input\"][i:i+batch_size].values.tolist()\n","                continuation = df[\"continuation\"][i:i+batch_size].values.tolist()\n","                prompt = [build_dialog(input[i] + continuation[i]) for i in range(len(input))]\n","\n","                out_text = generator.chat_completion(\n","                                                        prompt,\n","                                                        max_gen_len=1024,\n","                                                        temperature=1,\n","                                                        top_p=0.9\n","                                                    )\n","\n","                for j in range(len(out_text)):\n","                    _line = ''.join(filter(lambda x: x in p, input[j])).replace('\\n', '\\\\n').replace('\"', '\\\\\"')\n","                    _continuation = ''.join(filter(lambda x: x in p, continuation[j])).replace('\\n', '\\\\n').replace('\"', '\\\\\"')\n","                    _paraphrase = ''.join(filter(lambda x: x in p, out_text[j][\"generation\"][\"content\"][8:])).replace('\\n', '\\\\n').replace('\"', '\\\\\"')\n","\n","                    f.write(f'{{\"input\": \"{_line}\", \"continuation\": \"{_continuation}\", \"paraphrase\": \"{_paraphrase}\"}}\\n')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3100233,"status":"ok","timestamp":1713638134216,"user":{"displayName":"Alberto Almagro","userId":"03154998952450296226"},"user_tz":-60},"id":"f2rqNxqskdvF","outputId":"e963ef13-ad6a-4409-ecfb-c2cb102b2db8"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 117/117 [51:39<00:00, 26.49s/it]\n"]}],"source":["build_paraphrase(\"kirchenbauer_no_attack.json\", \"kirchenbauer_paraphrase.json\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6377963,"status":"ok","timestamp":1713618173372,"user":{"displayName":"Pepe Sánchez","userId":"07667389399180766726"},"user_tz":-60},"id":"rBAKrOG9qlll","outputId":"a42aa887-6c89-48dd-cd92-34c9676a1ae2"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 256/256 [1:46:17<00:00, 24.91s/it]\n"]}],"source":["build_paraphrase(\"kuditipudi_no_attack.json\", \"kuditipudi_paraphrase.json\")"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"sXZdjiXvqmzb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1713993270170,"user_tz":-60,"elapsed":591550,"user":{"displayName":"ALBERTO ALMAGRO SÁNCHEZ","userId":"05614599307350314709"}},"outputId":"1ab531e5-859b-4fea-cadb-819008261644"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 219/219 [1:43:52<00:00, 28.46s/it]\n"]}],"source":["build_paraphrase(\"wang_no_attack.json\", \"wang_paraphrase.json\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2323339,"status":"ok","timestamp":1713666070695,"user":{"displayName":"mikel sanchez","userId":"11069937688213907897"},"user_tz":-60},"id":"ILgcuyYFkknv","outputId":"237c107b-40b1-4c81-bd32-8e1e41a67dea"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 116/116 [52:55<00:00, 27.38s/it]\n"]}],"source":["build_paraphrase(\"opt_yang_no_attack.json\", \"opt_yang_paraphrase.json\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":677995,"status":"ok","timestamp":1713666748689,"user":{"displayName":"mikel sanchez","userId":"11069937688213907897"},"user_tz":-60},"id":"rdGjVRvskiJ1","outputId":"12e63485-7933-4d6c-afaa-ea54b62e4c6f"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 32/32 [11:13<00:00, 21.06s/it]\n"]}],"source":["build_paraphrase(\"llama_yang_no_attack.json\", \"llama_yang_paraphrase.json\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"jCw_z-cGBonm","outputId":"ce072067-2025-4dfd-c45a-997f37daa0a1"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 126/126 [1:01:53<00:00, 29.47s/it]\n"]}],"source":["build_paraphrase(\"zhao_no_attack.json\", \"zhao_paraphrase.json\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8l0YdulaGvWf"},"outputs":[],"source":["results = generator.chat_completion(\n","    [input],  # type: ignore\n","    max_gen_len=300,\n","    temperature=1,\n","    top_p=0.9,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16,"status":"ok","timestamp":1713576406675,"user":{"displayName":"mizudios monstie","userId":"17859370311270441790"},"user_tz":-60},"id":"qCOIieD5HJTR","outputId":"9333ca80-7ec4-4737-f0dc-ffffe527b47c"},"outputs":[{"data":{"text/plain":["[{'generation': {'role': 'assistant',\n","   'content': \" OUTPUT: When dealing with questions that lack clarity or accuracy, it's essential to provide thorough explanations rather than offering inaccurate responses. By doing so, you can help the asker understand the flaws in their question and find relevant information. If you're unsure about the answer, it's better to remain silent rather than providing false information that could potentially confuse or mislead. By maintaining the integrity of the question and the answer, you can build trust and ensure a more meaningful exchange.\"}}]"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["results"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}